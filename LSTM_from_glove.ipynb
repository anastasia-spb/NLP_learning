{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb2f5f2",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "---\n",
    "https://www.kaggle.com/competitions/nlp-txt-classification\n",
    "\n",
    "<br/>\n",
    "\n",
    "The goal is to classify tweets. There are 5 categories: Extremely Negative,  Negative, Neutral, Positive, Extremely Positive.\n",
    "\n",
    "This falls into the \"Classifying whole sentences\" category of common NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4e2b0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In this notebook I will use LSTM model with embeddings pretrained by GloVe (https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5936f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c813ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Cuda maintenance\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Torch device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322b343",
   "metadata": {},
   "source": [
    "### 0. Load data\n",
    "\n",
    "---\n",
    "\n",
    "In the cell below data are loaded from csv file and preprocessed:\n",
    "* Drop all rows where at least one NaN value is present\n",
    "* Drop all duplicates\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afd0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')  \n",
    "df_test = pd.read_csv('data/test.csv')  \n",
    "\n",
    "df = df.dropna().drop_duplicates().reset_index(drop=True)\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "df.rename(columns={\"Sentiment\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"Text\": \"text\"}, inplace=True)\n",
    "df = df.astype({\"text\": str}, {\"label\": str})\n",
    "\n",
    "labels = df[\"label\"].unique()\n",
    "num_labels = len(labels)\n",
    "\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: np.where(labels == x)[0][0])\n",
    "\n",
    "df_test = df_test.astype({\"Text\": str})\n",
    "df_test.rename(columns={\"Text\": \"text\"}, inplace=True)\n",
    "df_test = df_test.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de39d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...      0\n",
       "1  advice Talk to your neighbours family to excha...      1\n",
       "2  Coronavirus Australia: Woolworths to give elde...      1\n",
       "3  My food stock is not the only one which is emp...      1\n",
       "4  Me, ready to go at supermarket during the #COV...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7d8e2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Training data are split into training and evaluation parts:\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce91932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "eval_df.reset_index(drop=True, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f07c62",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now we can create three datasets. ``TextDataset`` inherits from ``torch.utils.data.Dataset``.\n",
    "And implements ``__getitem__`` function.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8781dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_dataset import TextDataset\n",
    "\n",
    "train_dataset = TextDataset(train_df)\n",
    "eval_dataset = TextDataset(eval_df)\n",
    "test_dataset = TextDataset(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84432b3e",
   "metadata": {},
   "source": [
    "### 1. Build vocabulary\n",
    "---\n",
    "<br/>\n",
    "\n",
    "As a next step we shall create a vocabulary from all tokens\n",
    "in all datasets (including the test one as well).\n",
    "\n",
    "I have decided to use **spacy** tokenizer, since it's the most popular rule-based tokenizer (according to https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "\n",
    "```\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "```\n",
    "\n",
    "However it gave accuracy only 0.28. Therefore I returned the \"basic_english\" tokenizer.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5275cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e7780a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  ?ÂThe past 20 years have been a rollercoaster for AfricaÂs #oil and #gas production. This 20-year up-and-down cycle coincided with oil prices,Â writes NJ Ayuk on #BillionsAtPlay. \r",
      "\r\n",
      "\r",
      "\r\n",
      "?Relevant as the world faces the impact of the #coronavirus!\r",
      "\r\n",
      "\r",
      "\r\n",
      "?: https://t.co/MWtz7Q28pP https://t.co/6bixG0ZGcW\n",
      "Tokenized: \n",
      "?, âthe, past, 20, years, have, been, a, rollercoaster, for, africaâs, #oil, and, #gas, production, ., this, 20-year, up-and-down, cycle, coincided, with, oil, prices, ,, â, writes, nj, ayuk, on, #billionsatplay, ., ?, relevant, as, the, world, faces, the, impact, of, the, #coronavirus, !, ?, https, //t, ., co/mwtz7q28pp, https, //t, ., co/6bixg0zgcw, "
     ]
    }
   ],
   "source": [
    "text_sample = train_dataset[0][\"text\"]\n",
    "tokens = tokenizer(text_sample)\n",
    "\n",
    "print(\"Original sentence: \", text_sample)\n",
    "print(\"Tokenized: \")\n",
    "for token in tokens:\n",
    "    print(token, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cca9ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44954lines [00:01, 36324.51lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "default_element = \"<UNK>\"\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for element in dataset:\n",
    "            yield tokenizer(element['text'].lower())\n",
    "    yield tokenizer(default_element) # Adding default element\n",
    "\n",
    "vocabulary = build_vocab_from_iterator(build_vocabulary([train_dataset,\n",
    "                                                         eval_dataset,\n",
    "                                                         test_dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835307f1",
   "metadata": {},
   "source": [
    "### 2. Load pretrained embeddings\n",
    "\n",
    "---\n",
    "<br/>\n",
    "\n",
    "Next step is to load pretrained **GloVe** model and extract embeddings from it.\n",
    "I have decided to use a model trained on twitter database, since we also work with tweets.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89fcc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_glove_twitter = api.load(\"glove-twitter-100\")\n",
    "weights = torch.FloatTensor(model_glove_twitter.vectors)\n",
    "emb_dim = weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ad8ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vector:  tensor([-0.0252,  0.1716,  0.7060, -0.0338,  0.4849, -0.1502, -0.1197,  0.4575,\n",
      "         0.2481, -0.5705, -0.3946,  0.4307, -4.3351,  0.2138, -0.3850, -0.6234,\n",
      "        -0.0492,  0.3994, -1.0027, -0.4547,  0.0878, -0.4888, -0.1139, -0.0667,\n",
      "         0.2869, -0.4383,  0.0987, -0.3731,  0.0329, -0.3718, -0.4841, -0.1208,\n",
      "        -0.1984, -0.0498,  0.4935,  0.2743,  0.6581, -0.0402, -0.1216,  1.2886,\n",
      "        -0.8873,  0.7126, -0.0645, -0.2075,  0.4908,  0.1899,  0.2216,  0.2641,\n",
      "        -0.0557,  0.6315, -0.5075, -0.0834, -0.1345,  0.0815, -0.4965,  0.1643,\n",
      "        -0.1437, -0.0217,  0.2549,  0.1717,  0.3381,  0.1570, -0.3156, -0.7458,\n",
      "         0.2387, -0.1820, -0.3221, -0.7053,  0.6782,  0.0383,  0.2351,  0.2206,\n",
      "         0.3867,  0.5412,  0.1290, -0.2550, -0.1341,  0.1612,  0.1051,  0.2181,\n",
      "         1.5972,  0.3996, -0.1259, -0.1610,  0.5371,  0.1436,  0.1532,  0.3488,\n",
      "         0.7011, -0.0236, -0.3139,  0.0303,  0.3886, -0.0444,  0.1591,  0.1538,\n",
      "         0.2294,  0.3795, -0.2286,  0.0414]) torch.Size([100])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# Lets test loaded vocab and embeggins mapping\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# Query\n",
    "query = 'home'\n",
    "if query in model_glove_twitter.vocab:\n",
    "    query_id = torch.tensor(model_glove_twitter.vocab[query].index)\n",
    "\n",
    "    embedding = nn.Embedding.from_pretrained(weights)\n",
    "    embedding.requires_grad = False\n",
    "\n",
    "    gensim_vector = torch.tensor(model_glove_twitter[query])\n",
    "    embedding_vector = embedding(query_id)\n",
    "\n",
    "    print(\"embedding_vector: \", embedding_vector, embedding_vector.shape)\n",
    "    print(gensim_vector == embedding_vector)\n",
    "else:\n",
    "    print(\"Not in vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb7413",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Next step is to construct matrix from pretrained embeggings:\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06d15c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((len(vocabulary), emb_dim))\n",
    "\n",
    "idx = 0\n",
    "not_pretrained_embeddings_count = 0\n",
    "for token in vocabulary.stoi:\n",
    "    token_str = token\n",
    "    if not isinstance(token_str, str):\n",
    "        token_str = token_str.text\n",
    "    if token_str in model_glove_twitter.vocab:\n",
    "        weights_matrix[idx] = torch.tensor(model_glove_twitter[token_str])\n",
    "    else:\n",
    "        weights_matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        not_pretrained_embeddings_count += 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d68c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68157 embeddings from 97237 are not pretrained\n"
     ]
    }
   ],
   "source": [
    "print(\"{} embeddings from {} are not pretrained\".format(not_pretrained_embeddings_count, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1954ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2860, -0.0493,  0.2279,  ..., -0.4137, -0.7897, -0.3616],\n",
       "        [ 0.6769,  0.2756,  1.1376,  ..., -0.0733,  0.1919,  0.4477],\n",
       "        [ 0.1821, -0.0485,  0.2397,  ..., -0.3358,  0.1888, -0.4079],\n",
       "        ...,\n",
       "        [-0.3959, -0.4705, -0.1837,  ...,  0.3289, -0.5314,  1.7251],\n",
       "        [ 0.3715, -0.7902, -0.2558,  ..., -0.6669, -1.7612, -0.3347],\n",
       "        [-0.3460, -1.0794, -0.7125,  ...,  0.1530,  0.0520, -0.5716]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Embeggins Layer\n",
    "\n",
    "num_embeddings, embedding_dim = weights_matrix.shape\n",
    "emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd831653",
   "metadata": {},
   "source": [
    "## 3. Create batches and DataLoaders\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next we shall create a function, which creates input data from batch.\n",
    "All vectors in batch shall be same size.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d03fbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCollator(object):\n",
    "    def __init__(self, vocabulary, tokenizer, max_words = 40, device = 'cuda'):\n",
    "        self.device = device\n",
    "        self.max_words = max_words\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __align__(self, tokens):\n",
    "        if len(tokens) < self.max_words:\n",
    "            return tokens + ([0]* (self.max_words-len(tokens)))\n",
    "        return tokens[:self.max_words]\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        X, Y = [], []\n",
    "        for element in batch:\n",
    "            tokenized_text = self.tokenizer(element[\"text\"].lower())\n",
    "            X.append([self.vocabulary[token] for token in tokenized_text])\n",
    "            Y.append(element['label'])\n",
    "        ## Bringing all samples to max_words length\n",
    "        X = [self.__align__(tokens) for tokens in X]\n",
    "        return torch.tensor(X, dtype=torch.int32, device=self.device), torch.tensor(Y, device=self.device) ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "757a1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_words = 40\n",
    "batch_collator = BatchCollator(vocabulary, tokenizer, max_words, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024,\n",
    "                          collate_fn=batch_collator, shuffle=True)\n",
    "eval_loader  = DataLoader(eval_dataset, batch_size=1024,\n",
    "                          collate_fn=batch_collator, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1024,\n",
    "                          collate_fn=batch_collator, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f98ffa",
   "metadata": {},
   "source": [
    "## 4. LSTM Model\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Finally we define two LSTM models:\n",
    "1. ``StackedLSTMClassifierWithPretrained`` - 3 Stacked LSTM layers\n",
    "2. ``LSTMClassifierWithPretrained`` - 3 LSTM layers in sequence\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e67db0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import gensim\n",
    "\n",
    "embed_len = emb_dim\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 = 60\n",
    "hidden_dim3 = 75\n",
    "n_layers = 1\n",
    "\n",
    "\n",
    "class StackedLSTMClassifierWithPretrained(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(StackedLSTMClassifierWithPretrained, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_layer = emb_layer\n",
    "        #self.embedding_layer.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim1, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim1, hidden_size=hidden_dim2, num_layers=1, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_dim2, hidden_size=hidden_dim3, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim3, num_labels)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim1, device=self.device), torch.randn(n_layers, len(X_batch), hidden_dim1, device=self.device)\n",
    "        output, (hidden, carry) = self.lstm1(embeddings, (hidden, carry))\n",
    "\n",
    "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim2, device=self.device), torch.randn(n_layers, len(X_batch), hidden_dim2, device=self.device)\n",
    "        output, (hidden, carry) = self.lstm2(output, (hidden, carry))\n",
    "\n",
    "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim3, device=self.device), torch.randn(n_layers, len(X_batch), hidden_dim3, device=self.device)\n",
    "        output, (hidden, carry) = self.lstm3(output, (hidden, carry))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "509e958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import gensim\n",
    "\n",
    "embed_len = emb_dim\n",
    "hidden_dim = 75\n",
    "n_layers=3\n",
    "\n",
    "\n",
    "class LSTMClassifierWithPretrained(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(LSTMClassifierWithPretrained, self).__init__()\n",
    "        self.embedding_layer = emb_layer\n",
    "        #self.embedding_layer.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, \n",
    "                              num_layers=n_layers, batch_first=True)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        output, (hidden, carry) = self.lstm(embeddings)\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b62886",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "For both models:\n",
    "\n",
    "* Loss function: CrossEntropyLoss\n",
    "* Optimizer ADAM: Adam\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5501f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).cpu().mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.cpu().detach().numpy(), \n",
    "                                                          Y_preds.cpu().detach().numpy())))\n",
    "\n",
    "\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, device, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            X.to(device)\n",
    "            Y.to(device)\n",
    "            model.to(device)\n",
    "            Y_preds = model(X) ## Make Predictions\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y) ## Calculate Loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad() ## Clear previously calculated gradients\n",
    "            loss.backward() ## Calculates Gradients\n",
    "            optimizer.step() ## Update network weights.\n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbccc216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.542\n",
      "Valid Loss : 1.511\n",
      "Valid Acc  : 0.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.196\n",
      "Valid Loss : 1.502\n",
      "Valid Acc  : 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.795\n",
      "Valid Loss : 1.782\n",
      "Valid Acc  : 0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.606\n",
      "Valid Loss : 1.939\n",
      "Valid Acc  : 0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.459\n",
      "Valid Loss : 1.994\n",
      "Valid Acc  : 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.329\n",
      "Valid Loss : 1.872\n",
      "Valid Acc  : 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.244\n",
      "Valid Loss : 1.894\n",
      "Valid Acc  : 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.170\n",
      "Valid Loss : 1.957\n",
      "Valid Acc  : 0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.123\n",
      "Valid Loss : 1.965\n",
      "Valid Acc  : 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.096\n",
      "Valid Loss : 2.240\n",
      "Valid Acc  : 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.085\n",
      "Valid Loss : 2.219\n",
      "Valid Acc  : 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.111\n",
      "Valid Loss : 2.171\n",
      "Valid Acc  : 0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.080\n",
      "Valid Loss : 2.151\n",
      "Valid Acc  : 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.065\n",
      "Valid Loss : 2.302\n",
      "Valid Acc  : 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.057\n",
      "Valid Loss : 2.341\n",
      "Valid Acc  : 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.061\n",
      "Valid Loss : 2.415\n",
      "Valid Acc  : 0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.059\n",
      "Valid Loss : 2.533\n",
      "Valid Acc  : 0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.049\n",
      "Valid Loss : 2.481\n",
      "Valid Acc  : 0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.050\n",
      "Valid Loss : 2.567\n",
      "Valid Acc  : 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.113\n",
      "Valid Loss : 2.177\n",
      "Valid Acc  : 0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.058\n",
      "Valid Loss : 2.431\n",
      "Valid Acc  : 0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.044\n",
      "Valid Loss : 2.482\n",
      "Valid Acc  : 0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.041\n",
      "Valid Loss : 2.547\n",
      "Valid Acc  : 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.039\n",
      "Valid Loss : 2.594\n",
      "Valid Acc  : 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.032\n",
      "Valid Loss : 2.576\n",
      "Valid Acc  : 0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.031\n",
      "Valid Loss : 2.724\n",
      "Valid Acc  : 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.032\n",
      "Valid Loss : 2.725\n",
      "Valid Acc  : 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.030\n",
      "Valid Loss : 2.772\n",
      "Valid Acc  : 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.027\n",
      "Valid Loss : 2.721\n",
      "Valid Acc  : 0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.028\n",
      "Valid Loss : 2.737\n",
      "Valid Acc  : 0.525\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_classifier = LSTMClassifierWithPretrained(device)\n",
    "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(lstm_classifier, loss_fn, optimizer, train_loader, eval_loader, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc672927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.551\n",
      "Valid Loss : 1.526\n",
      "Valid Acc  : 0.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.504\n",
      "Valid Loss : 1.500\n",
      "Valid Acc  : 0.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.325\n",
      "Valid Loss : 1.381\n",
      "Valid Acc  : 0.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.103\n",
      "Valid Loss : 1.486\n",
      "Valid Acc  : 0.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.011\n",
      "Valid Loss : 1.499\n",
      "Valid Acc  : 0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.960\n",
      "Valid Loss : 1.526\n",
      "Valid Acc  : 0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.831\n",
      "Valid Loss : 1.313\n",
      "Valid Acc  : 0.510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.578\n",
      "Valid Loss : 1.276\n",
      "Valid Acc  : 0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.409\n",
      "Valid Loss : 1.288\n",
      "Valid Acc  : 0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.349\n",
      "Valid Loss : 1.304\n",
      "Valid Acc  : 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.284\n",
      "Valid Loss : 1.321\n",
      "Valid Acc  : 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.250\n",
      "Valid Loss : 1.351\n",
      "Valid Acc  : 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.219\n",
      "Valid Loss : 1.446\n",
      "Valid Acc  : 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.200\n",
      "Valid Loss : 1.502\n",
      "Valid Acc  : 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.185\n",
      "Valid Loss : 1.587\n",
      "Valid Acc  : 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.170\n",
      "Valid Loss : 1.584\n",
      "Valid Acc  : 0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.162\n",
      "Valid Loss : 1.562\n",
      "Valid Acc  : 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.149\n",
      "Valid Loss : 1.695\n",
      "Valid Acc  : 0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.143\n",
      "Valid Loss : 1.740\n",
      "Valid Acc  : 0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.146\n",
      "Valid Loss : 1.716\n",
      "Valid Acc  : 0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.126\n",
      "Valid Loss : 1.706\n",
      "Valid Acc  : 0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.124\n",
      "Valid Loss : 1.798\n",
      "Valid Acc  : 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.114\n",
      "Valid Loss : 1.840\n",
      "Valid Acc  : 0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.108\n",
      "Valid Loss : 1.827\n",
      "Valid Acc  : 0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.099\n",
      "Valid Loss : 1.836\n",
      "Valid Acc  : 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.108\n",
      "Valid Loss : 1.802\n",
      "Valid Acc  : 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.100\n",
      "Valid Loss : 1.898\n",
      "Valid Acc  : 0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.097\n",
      "Valid Loss : 1.985\n",
      "Valid Acc  : 0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.081\n",
      "Valid Loss : 2.060\n",
      "Valid Acc  : 0.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:01<00:00, 19.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.077\n",
      "Valid Loss : 1.943\n",
      "Valid Acc  : 0.585\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_classifier = StackedLSTMClassifierWithPretrained(device)\n",
    "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(lstm_classifier, loss_fn, optimizer, train_loader, eval_loader, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3336ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePredictions(model, loader, device):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        X.to(device)\n",
    "        Y.to(device)\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy()\n",
    "\n",
    "Y_actual, Y_preds = MakePredictions(lstm_classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "446732eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b130f7fb-7048-48e6-a8af-57bb56ac1e27</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>db72c632-8719-4847-b7f2-a89af05e1504</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e45239d8-4dcf-4685-a955-a9a08ca829ee</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2854b1b2-5a41-4002-90d3-17fe77a3a78e</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ff9be7e1-81a9-4c07-beda-4fee9a923f5e</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id           Sentiment\n",
       "0  787bc85b-20d4-46d8-84a0-562a2527f684            Negative\n",
       "1  17e934cd-ba94-4d4f-9ac0-ead202abe241            Negative\n",
       "2  5914534b-2b0f-4de8-bb8a-e25587697e0d  Extremely Positive\n",
       "3  cdf06cfe-29ae-48ee-ac6d-be448103ba45  Extremely Negative\n",
       "4  aff63979-0256-4fb9-a2d9-86a3d3ca5470            Positive\n",
       "5  b130f7fb-7048-48e6-a8af-57bb56ac1e27             Neutral\n",
       "6  db72c632-8719-4847-b7f2-a89af05e1504            Negative\n",
       "7  e45239d8-4dcf-4685-a955-a9a08ca829ee             Neutral\n",
       "8  2854b1b2-5a41-4002-90d3-17fe77a3a78e            Positive\n",
       "9  ff9be7e1-81a9-4c07-beda-4fee9a923f5e            Positive"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission.head()\n",
    "\n",
    "sample_submission['Sentiment'] = [labels[pred] for pred in Y_preds]\n",
    "sample_submission.to_csv('lstm_test_submission.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4f368",
   "metadata": {},
   "source": [
    "## Result:\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "Evaluation on test dataset: 0.487\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab1c92",
   "metadata": {},
   "source": [
    "## Useful Links:\n",
    "\n",
    "https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-lstm-for-text-classification-tasks#4\n",
    "    \n",
    "A Comprehensive Guide to Understand and Implement Text Classification in Python:\n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "How to Use GloVe Word Embeddings With PyTorch Networks?\n",
    "https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch\n",
    "\n",
    "How to use Pre-trained Word Embeddings in PyTorch\n",
    "https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94d6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
