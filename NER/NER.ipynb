{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb2f5f2",
   "metadata": {},
   "source": [
    "## Token Classification\n",
    "\n",
    "---\n",
    "\n",
    "Within this task token classification model shall be applied on provided dataset.\n",
    "Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n",
    "\n",
    "\n",
    "| Abbreviation |Description  |\n",
    "|---|---|\n",
    "| O     | Outside of a named entity                                      |\n",
    "| B-ORG | Beginning of an organization right after another organization  |\n",
    "| I-ORG | organization                                                   |\n",
    "| B-LOC | Beginning of a location right after another location           |\n",
    "| B-PER | Beginning of a person’s name right after another person’s name |\n",
    "| I-PER | Person’s name                                                  |\n",
    "| I-LOC | Location                                                       |\n",
    "\n",
    "Source: https://huggingface.co/dslim/bert-base-NER?text=My+name+is+Wolfgang+and+I+live+in+Berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e93c5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5936f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c813ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Cuda maintenance\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Torch device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322b343",
   "metadata": {},
   "source": [
    "### 0. Load data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "60ea024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import create_df_from_text_file\n",
    "\n",
    "df_dev = create_df_from_text_file('data/dev.txt')\n",
    "df_train = create_df_from_text_file('data/train.txt')\n",
    "df_test = create_df_from_text_file('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f27ab182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[как, акционерный, коммерческий, Московский, м...</td>\n",
       "      <td>[O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Управлять, ЦАО, и, САО, вместо, Алексея, Алек...</td>\n",
       "      <td>[O, B-LOC, O, B-LOC, O, B-PER, I-PER, O, B-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[О, задержании, Шакирьянова, стало, известно, ...</td>\n",
       "      <td>[O, O, B-PER, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[После, майского, ухода, вице-премьера, Владис...</td>\n",
       "      <td>[O, O, O, O, B-PER, I-PER, O, O, O, O, O, B-PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Армяне, со, мной, согласились, ,, с, Ильхамом...</td>\n",
       "      <td>[O, O, O, O, O, O, B-PER, I-PER, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [как, акционерный, коммерческий, Московский, м...   \n",
       "1  [Управлять, ЦАО, и, САО, вместо, Алексея, Алек...   \n",
       "2  [О, задержании, Шакирьянова, стало, известно, ...   \n",
       "3  [После, майского, ухода, вице-премьера, Владис...   \n",
       "4  [Армяне, со, мной, согласились, ,, с, Ильхамом...   \n",
       "\n",
       "                                               label  \n",
       "0  [O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I...  \n",
       "1  [O, B-LOC, O, B-LOC, O, B-PER, I-PER, O, B-PER...  \n",
       "2                 [O, O, B-PER, O, O, O, O, O, O, O]  \n",
       "3  [O, O, O, O, B-PER, I-PER, O, O, O, O, O, B-PE...  \n",
       "4  [O, O, O, O, O, O, B-PER, I-PER, O, O, O, O, O...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "95882529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2idx(labels):\n",
    "    return [label_names.index(label) for label in labels]\n",
    "\n",
    "df_dev['label'] = df_dev['label'].apply(lambda labels: labels2idx(labels))\n",
    "df_train['label'] = df_train['label'].apply(lambda labels: labels2idx(labels))\n",
    "df_test['label'] = df_test['label'].apply(lambda labels: labels2idx(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e5bca99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(df_train.to_dict('list'))\n",
    "eval_dataset = Dataset.from_dict(df_dev.to_dict('list'))\n",
    "test_dataset = Dataset.from_dict(df_test.to_dict('list'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575ad95",
   "metadata": {},
   "source": [
    "### 1. Choose model\n",
    "\n",
    "----\n",
    "\n",
    "**Model Description**\n",
    "<br/>\n",
    "\n",
    "**Summary**: <br/>\n",
    "mBERT model fine-tuned for 3 epochs on the recently-introduced WikiNEuRal dataset for Multilingual NER. The system supports the 9 languages covered by WikiNEuRal (de, en, es, fr, it, nl, pl, pt, ru), and it was trained on all 9 languages jointly. For a stronger baseline system (mBERT + Bi-LSTM + CRF) look at the official repository.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Official Repository: https://github.com/Babelscape/wikineural <br/>\n",
    "Paper: https://aclanthology.org/wikineural\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c11fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Babelscape/wikineural-multilingual-ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9ee824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'Если', 'Миронов', 'занял', 'столь', 'оппозиционную', 'позицию', ',', 'то', 'мне', 'представляется', ',', 'что', 'для', 'него', 'было', 'бы', 'порядочным', 'и', 'правильным', 'уйти', 'в', 'отставку', 'с', 'занимаемого', 'им', 'поста', ',', 'поста', ',', 'который', 'предоставлен', 'ему', 'сегодня', '\"', 'Единой', 'Россией', \"''\", 'и', 'никем', 'больше', \"''\", ',', '-', 'заключает', 'Исаев', '.']\n",
      "['[CLS]', '\"', 'Если', 'Мир', '##онов', 'занял', 'сто', '##ль', 'о', '##п', '##по', '##зици', '##он', '##ную', 'позицию', ',', 'то', 'мне', 'представляет', '##ся', ',', 'что', 'для', 'него', 'было', 'бы', 'пор', '##яд', '##о', '##чным', 'и', 'правил', '##ьным', 'у', '##йти', 'в', 'отставку', 'с', 'за', '##нима', '##емого', 'им', 'поста', ',', 'поста', ',', 'который', 'пред', '##ост', '##ав', '##лен', 'ему', 'сегодня', '\"', 'Е', '##дино', '##й', 'Р', '##ос', '##сией', \"'\", \"'\", 'и', 'ни', '##ке', '##м', 'больше', \"'\", \"'\", ',', '-', 'за', '##кл', '##ю', '##чает', 'И', '##са', '##ев', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Show tokenizer output\n",
    "\n",
    "train_example = df_train.tokens[0]\n",
    "train_example_labels = df_train.label[0]\n",
    "tokenized_input = tokenizer(train_example, is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(train_example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab256083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c439e5d83f4049a5834599f27ab965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0227a70ceff9495fb612608c53a35e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizer_utils import align_labels_with_tokens, tokenize_and_align_labels\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# As a next step we have to align tokens generated \n",
    "# by retrained AutoTokenizer with labels_names (ner_tagss))\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels, batched=True,\n",
    "    remove_columns=[\"tokens\", \"label\"],\n",
    ")\n",
    "\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    tokenize_and_align_labels, batched=True,\n",
    "    remove_columns=[\"tokens\", \"label\"]\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels, batched=True,\n",
    "    remove_columns=[\"tokens\", \"label\"]\n",
    ")\n",
    "\n",
    "# Data Collator is used to create a batch of examples.\n",
    "# It will dynamically pad text to the length of the longest element in a batch\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ed6d5bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   101,    107,  33463,  53275,  45568,  40555, 108804,  12118,    555,\n",
       "           11078,  53204,  92522,  11579,  12719, 110028,    117,  11663,  67251,\n",
       "           36932,  10625,    117,  10791,  10520,  13981,  11582,  22504,  41436,\n",
       "           35528,  10316,  27819,    549,  75529,  30982,    560,  37756,    543,\n",
       "           75376,    558,  10234,  97582,  91024,  13327,  80765,    117,  80765,\n",
       "             117,  12968,  23807,  33580,  18197,  16173,  16929,  72166,    107,\n",
       "             514, 105088,  10384,    525,  17969, 106801,    112,    112,    549,\n",
       "           19544,  11557,  10241,  26368,    112,    112,    117,    118,  10234,\n",
       "           53869,  10593,  52928,    517,  12016,  13292,    119,    102,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0],\n",
       "         [   101,  87653,  40383,    107,    535,    112,    112,    543,  84043,\n",
       "           20007,  10746,  18746, 108804,  70667,  10384,    553,  82594,  10191,\n",
       "           84370,  33489,  32418,  81802,  11692,    117,  10791,  11495,  10513,\n",
       "           53204,  60539,  84078,  28033,  13826,  10122,  45572,  49964,    558,\n",
       "           35189,    107,  20594,  29749,  18617,  93503,    558,  23600, 102006,\n",
       "           13036,  20554,  10241,    112,    112,    119,    102,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0]]),\n",
       " tensor([[-100,    0,    0,    1,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    4,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    1,    2,    2,    0, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    3,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    1,    2,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    5,    6,    6,    6,    6,\n",
       "             0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100]]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data collator\n",
    "batch_example = data_collator([tokenized_train_dataset[i] for i in range(2)])\n",
    "batch_example[\"input_ids\"], batch_example[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1890808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_utils import compute_metrics\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf1cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "id2label = {str(i): label for i, label in enumerate(labels_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint,\n",
    "                                                       id2label=id2label,\n",
    "                                                       label2id=label2id,\n",
    "                                                       ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5be332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e0ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nastya/anaconda3/envs/env_torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7746\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5811\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5811' max='5811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5811/5811 15:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.039813</td>\n",
       "      <td>0.948772</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>0.952308</td>\n",
       "      <td>0.990404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.037016</td>\n",
       "      <td>0.963230</td>\n",
       "      <td>0.975058</td>\n",
       "      <td>0.969108</td>\n",
       "      <td>0.992298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.036634</td>\n",
       "      <td>0.964136</td>\n",
       "      <td>0.969685</td>\n",
       "      <td>0.966903</td>\n",
       "      <td>0.992750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2582\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f7ea4ad41f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1937\n",
      "Configuration saved in ./results/checkpoint-1937/config.json\n",
      "Model weights saved in ./results/checkpoint-1937/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1937/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1937/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2582\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f7ea476ad30>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-3874\n",
      "Configuration saved in ./results/checkpoint-3874/config.json\n",
      "Model weights saved in ./results/checkpoint-3874/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3874/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3874/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2582\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f7ea476a8e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-5811\n",
      "Configuration saved in ./results/checkpoint-5811/config.json\n",
      "Model weights saved in ./results/checkpoint-5811/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5811/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5811/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5811, training_loss=0.02255878651989366, metrics={'train_runtime': 937.663, 'train_samples_per_second': 24.783, 'train_steps_per_second': 6.197, 'total_flos': 2674150607863404.0, 'train_loss': 0.02255878651989366, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708312dd",
   "metadata": {},
   "source": [
    "### 2. Evaluate on test dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b903d4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2582\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='646' max='646' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [646/646 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f7ea47b75b0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03453731909394264,\n",
       " 'eval_precision': 0.9672398190045249,\n",
       " 'eval_recall': 0.974115931461903,\n",
       " 'eval_f1': 0.9706656979384253,\n",
       " 'eval_accuracy': 0.9930352501624431,\n",
       " 'eval_runtime': 29.1135,\n",
       " 'eval_samples_per_second': 88.688,\n",
       " 'eval_steps_per_second': 22.189,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4070e",
   "metadata": {},
   "source": [
    "### 3. Load trained model and check on random sample\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99cebc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "trained_model = AutoModelForTokenClassification.from_pretrained(\"./results/checkpoint-5811\",\n",
    "                                                       id2label=id2label,\n",
    "                                                       label2id=label2id,\n",
    "                                                       ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9ad4ff2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Молчаливое': {'Tokens': ['М', '##ол', '##чали', '##вое'],\n",
       "  'Labels': ['O', 'O', 'O', 'O']},\n",
       " 'утопическе': {'Tokens': ['у', '##то', '##пи', '##че', '##ске'],\n",
       "  'Labels': ['O', 'O', 'O', 'O', 'O']},\n",
       " 'представление': {'Tokens': ['представлен', '##ие'], 'Labels': ['O', 'O']},\n",
       " 'о': {'Tokens': ['о'], 'Labels': ['O']},\n",
       " 'Вавилоне': {'Tokens': ['В', '##ави', '##лон', '##е'],\n",
       "  'Labels': ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']},\n",
       " 'в': {'Tokens': ['в'], 'Labels': ['O']},\n",
       " 'Поднебесной.': {'Tokens': ['Под', '##не', '##бе', '##сной', '.'],\n",
       "  'Labels': ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O']}}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_string = \"Больше всего новых машин купили в Казани (57%), Набережных Челнах (54%), Тюмени (47%), Ульяновске и Самаре (по 38%).\"\n",
    "test_string = \"Молчаливое утопическе представление о Вавилоне в Поднебесной.\"\n",
    "tokenized_input = tokenizer(test_string.split(), is_split_into_words=True, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
    "predictions = model(tokenized_input[\"input_ids\"])\n",
    "predictions = np.argmax(predictions[\"logits\"].detach().numpy(), axis=-1)\n",
    "result = format_result(test_string, tokens, predictions, tokenized_input, labels_names)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd2838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
